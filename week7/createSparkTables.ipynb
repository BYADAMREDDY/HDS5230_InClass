{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c6160f-7ccc-4228-806a-d53bb8b6af8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "# LINK to DATABRICKS - https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1496180313079957/3631613044829155/1978268099723113/latest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa12afa-2bb4-4231-b974-cab6d81821eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully as PGYR2023_P01302025_01212025.zip\n"
     ]
    }
   ],
   "source": [
    "url = \"https://download.cms.gov/openpayments/PGYR2023_P01302025_01212025.zip\"\n",
    "local_filename = url.split(\"/\")[-1] # Extracts the file name from the URL\n",
    "buffer = io.BytesIO()\n",
    "buffer.seek(0)\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "# response = requests.get(url2, stream=True)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            buffer.write(chunk)\n",
    "    print(f\"File downloaded successfully as {local_filename}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b244d365-4f30-4452-8c28-a61312a969b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-3631613044829181>:12\u001b[0m\n",
       "\u001b[1;32m     10\u001b[0m zip_ref\u001b[38;5;241m.\u001b[39mextract(file_name, temp_dir)\n",
       "\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
       "\u001b[0;32m---> 12\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(table_name)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1520\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n",
       "\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "\u001b[1;32m   1519\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n",
       "\u001b[0;32m-> 1520\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
       "\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o772.saveAsTable.\n",
       ": org.apache.spark.SparkRuntimeException: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`op_dtl_ownrshp_pgyr2023_p01302025_01212025`, as its associated location 'dbfs:/user/hive/warehouse/op_dtl_ownrshp_pgyr2023_p01302025_01212025' already exists. Please pick a different table name, or remove the existing location first.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.locationAlreadyExists(QueryExecutionErrors.scala:2999)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalogImpl.validateTableLocation(SessionCatalog.scala:947)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.validateTableLocation(ManagedCatalogSessionCatalog.scala:894)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:180)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:805)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:782)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:659)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\nFile \u001b[0;32m<command-3631613044829181>:12\u001b[0m\n\u001b[1;32m     10\u001b[0m zip_ref\u001b[38;5;241m.\u001b[39mextract(file_name, temp_dir)\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(table_name)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1520\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1520\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o772.saveAsTable.\n: org.apache.spark.SparkRuntimeException: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`op_dtl_ownrshp_pgyr2023_p01302025_01212025`, as its associated location 'dbfs:/user/hive/warehouse/op_dtl_ownrshp_pgyr2023_p01302025_01212025' already exists. Please pick a different table name, or remove the existing location first.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.locationAlreadyExists(QueryExecutionErrors.scala:2999)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalogImpl.validateTableLocation(SessionCatalog.scala:947)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.validateTableLocation(ManagedCatalogSessionCatalog.scala:894)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:180)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:805)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:782)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:659)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "org.apache.spark.SparkRuntimeException: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`op_dtl_ownrshp_pgyr2023_p01302025_01212025`, as its associated location 'dbfs:/user/hive/warehouse/op_dtl_ownrshp_pgyr2023_p01302025_01212025' already exists. Please pick a different table name, or remove the existing location first.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_dir = os.getcwd()\n",
    "buffer.seek(0)\n",
    "\n",
    "with zipfile.ZipFile(buffer, 'r') as zip_ref:\n",
    "    # Loop through each file in the ZIP archive\n",
    "    for file_name in zip_ref.namelist():\n",
    "        table_name,ext = os.path.splitext(file_name)\n",
    "        if ext.lower() == \".csv\":\n",
    "            temp_path = os.path.join(temp_dir, file_name)\n",
    "            zip_ref.extract(file_name, temp_dir)\n",
    "            df = spark.read.csv(f\"file:{temp_path}\", header=True, inferSchema=True)\n",
    "            df.write.format(\"parquet\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04a97ce-a97a-46cb-9867-a15014868ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "url2 = \"https://download.cms.gov/openpayments/PHPRFL_P01302025_01212025.zip\"\n",
    "local_filename = url.split(\"/\")[-1] # Extracts the file name from the URL\n",
    "buffer = io.BytesIO()\n",
    "buffer.seek(0)\n",
    "\n",
    "response = requests.get(url2, stream=True)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            buffer.write(chunk)\n",
    "    print(f\"File downloaded successfully as {local_filename}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e881e4-c1ec-495a-a968-44350603a2f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_dir = os.getcwd()\n",
    "buffer.seek(0)\n",
    "\n",
    "with zipfile.ZipFile(buffer, 'r') as zip_ref:\n",
    "    # Loop through each file in the ZIP archive\n",
    "    for file_name in zip_ref.namelist():\n",
    "        table_name,ext = os.path.splitext(file_name)\n",
    "        if ext.lower() == \".csv\":\n",
    "            temp_path = os.path.join(temp_dir, file_name)\n",
    "            zip_ref.extract(file_name, temp_dir)\n",
    "            df = spark.read.csv(f\"file:{temp_path}\", header=True, inferSchema=True)\n",
    "            df.write.format(\"parquet\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53dc856d-52f7-46b7-b968-46243eb21233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.table(f\"default.op_dtl_gnrl_pgyr2023_p01302025_01212025\")\n",
    "# df2 = spark.read.table(f\"default.OP_CVRD_RCPNT_PRFL_SPLMTL_P01302025_01212025\")\n",
    "\n",
    "PATH = \"dbfs:/user/hive/warehouse/op_dtl_gnrl_pgyr2023_p01302025_01212025\"\n",
    "PATH2 = \"dbfs:/user/hive/warehouse/op_cvrd_rcpnt_prfl_splmtl_p01302025_01212025\"\n",
    "df = spark.read.format('parquet').load(PATH)\n",
    "df2 = spark.read.format('parquet').load(PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ded8aad-dba5-413f-9f53-5be8372f056a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: [StructField('Change_Type', StringType(), True),\n",
      " StructField('Covered_Recipient_Type', StringType(), True),\n",
      " StructField('Teaching_Hospital_CCN', StringType(), True),\n",
      " StructField('Teaching_Hospital_ID', StringType(), True),\n",
      " StructField('Teaching_Hospital_Name', StringType(), True),\n",
      " StructField('Covered_Recipient_Profile_ID', StringType(), True),\n",
      " StructField('Covered_Recipient_NPI', StringType(), True),\n",
      " StructField('Covered_Recipient_First_Name', StringType(), True),\n",
      " StructField('Covered_Recipient_Middle_Name', StringType(), True),\n",
      " StructField('Covered_Recipient_Last_Name', StringType(), True),\n",
      " StructField('Covered_Recipient_Name_Suffix', StringType(), True),\n",
      " StructField('Recipient_Primary_Business_Street_Address_Line1', StringType(), True),\n",
      " StructField('Recipient_Primary_Business_Street_Address_Line2', StringType(), True),\n",
      " StructField('Recipient_City', StringType(), True),\n",
      " StructField('Recipient_State', StringType(), True),\n",
      " StructField('Recipient_Zip_Code', StringType(), True),\n",
      " StructField('Recipient_Country', StringType(), True),\n",
      " StructField('Recipient_Province', StringType(), True),\n",
      " StructField('Recipient_Postal_Code', StringType(), True),\n",
      " StructField('Covered_Recipient_Primary_Type_1', StringType(), True),\n",
      " StructField('Covered_Recipient_Primary_Type_2', StringType(), True),\n",
      " StructField('Covered_Recipient_Primary_Type_3', StringType(), True),\n",
      " StructField('Covered_Recipient_Primary_Type_4', StringType(), True),\n",
      " StructField('Covered_Recipient_Primary_Type_5', StringType(), True),\n",
      " StructField('Covered_Recipient_Primary_Type_6', StringType(), True),\n",
      " StructField('Covered_Recipient_Specialty_1', StringType(), True),\n",
      " StructField('Covered_Recipient_Specialty_2', StringType(), True),\n",
      " StructField('Covered_Recipient_Specialty_3', StringType(), True),\n",
      " StructField('Covered_Recipient_Specialty_4', StringType(), True),\n",
      " StructField('Covered_Recipient_Specialty_5', StringType(), True),\n",
      " StructField('Covered_Recipient_Specialty_6', StringType(), True),\n",
      " StructField('Covered_Recipient_License_State_code1', StringType(), True),\n",
      " StructField('Covered_Recipient_License_State_code2', StringType(), True),\n",
      " StructField('Covered_Recipient_License_State_code3', StringType(), True),\n",
      " StructField('Covered_Recipient_License_State_code4', StringType(), True),\n",
      " StructField('Covered_Recipient_License_State_code5', StringType(), True),\n",
      " StructField('Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name', StringType(), True),\n",
      " StructField('Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_ID', StringType(), True),\n",
      " StructField('Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name', StringType(), True),\n",
      " StructField('Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_State', StringType(), True),\n",
      " StructField('Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Country', StringType(), True),\n",
      " StructField('Total_Amount_of_Payment_USDollars', StringType(), True),\n",
      " StructField('Date_of_Payment', StringType(), True),\n",
      " StructField('Number_of_Payments_Included_in_Total_Amount', StringType(), True),\n",
      " StructField('Form_of_Payment_or_Transfer_of_Value', StringType(), True),\n",
      " StructField('Nature_of_Payment_or_Transfer_of_Value', StringType(), True),\n",
      " StructField('City_of_Travel', StringType(), True),\n",
      " StructField('State_of_Travel', StringType(), True),\n",
      " StructField('Country_of_Travel', StringType(), True),\n",
      " StructField('Physician_Ownership_Indicator', StringType(), True),\n",
      " StructField('Third_Party_Payment_Recipient_Indicator', StringType(), True),\n",
      " StructField('Name_of_Third_Party_Entity_Receiving_Payment_or_Transfer_of_Value', StringType(), True),\n",
      " StructField('Charity_Indicator', StringType(), True),\n",
      " StructField('Third_Party_Equals_Covered_Recipient_Indicator', StringType(), True),\n",
      " StructField('Contextual_Information', StringType(), True),\n",
      " StructField('Delay_in_Publication_Indicator', StringType(), True),\n",
      " StructField('Record_ID', StringType(), True),\n",
      " StructField('Dispute_Status_for_Publication', StringType(), True),\n",
      " StructField('Related_Product_Indicator', StringType(), True),\n",
      " StructField('Covered_or_Noncovered_Indicator_1', StringType(), True),\n",
      " StructField('Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_1', StringType(), True),\n",
      " StructField('Product_Category_or_Therapeutic_Area_1', StringType(), True),\n",
      " StructField('Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_1', StringType(), True),\n",
      " StructField('Associated_Drug_or_Biological_NDC_1', StringType(), True),\n",
      " StructField('Associated_Device_or_Medical_Supply_PDI_1', StringType(), True),\n",
      " StructField('Covered_or_Noncovered_Indicator_2', StringType(), True),\n",
      " StructField('Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_2', StringType(), True),\n",
      " StructField('Product_Category_or_Therapeutic_Area_2', StringType(), True),\n",
      " StructField('Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_2', StringType(), True),\n",
      " StructField('Associated_Drug_or_Biological_NDC_2', StringType(), True),\n",
      " StructField('Associated_Device_or_Medical_Supply_PDI_2', StringType(), True),\n",
      " StructField('Covered_or_Noncovered_Indicator_3', StringType(), True),\n",
      " StructField('Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_3', StringType(), True),\n",
      " StructField('Product_Category_or_Therapeutic_Area_3', StringType(), True),\n",
      " StructField('Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_3', StringType(), True),\n",
      " StructField('Associated_Drug_or_Biological_NDC_3', StringType(), True),\n",
      " StructField('Associated_Device_or_Medical_Supply_PDI_3', StringType(), True),\n",
      " StructField('Covered_or_Noncovered_Indicator_4', StringType(), True),\n",
      " StructField('Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_4', StringType(), True),\n",
      " StructField('Product_Category_or_Therapeutic_Area_4', StringType(), True),\n",
      " StructField('Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_4', StringType(), True),\n",
      " StructField('Associated_Drug_or_Biological_NDC_4', StringType(), True),\n",
      " StructField('Associated_Device_or_Medical_Supply_PDI_4', StringType(), True),\n",
      " StructField('Covered_or_Noncovered_Indicator_5', StringType(), True),\n",
      " StructField('Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_5', StringType(), True),\n",
      " StructField('Product_Category_or_Therapeutic_Area_5', StringType(), True),\n",
      " StructField('Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_5', StringType(), True),\n",
      " StructField('Associated_Drug_or_Biological_NDC_5', StringType(), True),\n",
      " StructField('Associated_Device_or_Medical_Supply_PDI_5', StringType(), True),\n",
      " StructField('Program_Year', IntegerType(), True),\n",
      " StructField('Payment_Publication_Date', StringType(), True)]"
     ]
    }
   ],
   "source": [
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f964d7d3-c4e0-4eff-a82e-e7d5fc96bb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Total_Amount_of_Payment_USDollars\", F.col(\"Total_Amount_of_Payment_USDollars\").cast(T.FloatType()))\n",
    "\n",
    "filtered_df = df.filter(F.col(\"Total_Amount_of_Payment_USDollars\") > 1000)\n",
    "\n",
    "g_df = filtered_df.groupBy([F.col(\"Nature_of_Payment_or_Transfer_of_Value\")]).agg(\n",
    "    F.count(F.col(\"Total_Amount_of_Payment_USDollars\")).alias(\"count\"),\n",
    "    F.sum(F.col(\"Total_Amount_of_Payment_USDollars\")).alias(\"total_amount_sum\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e372203-426a-4ebd-97a4-9626a44afc51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------+--------------------+\n",
      "|Nature_of_Payment_or_Transfer_of_Value| count|    total_amount_sum|\n",
      "+--------------------------------------+------+--------------------+\n",
      "|                  Compensation for ...|164093| 5.582959709562378E8|\n",
      "|                        Consulting Fee|105239| 4.819268881451416E8|\n",
      "|                    Travel and Lodging| 24793| 5.476596304553223E7|\n",
      "|                             Honoraria| 13750| 5.311015340332031E7|\n",
      "|                             Education| 13376| 5.179660454785156E7|\n",
      "|                    Royalty or License| 11538| 1.190650138420227E9|\n",
      "|                  Compensation for ...|  8658| 2.762957946057129E7|\n",
      "|                                 Grant|  4922|1.1127899889367676E8|\n",
      "|                  Space rental or f...|  4917| 2.768891679724121E7|\n",
      "|                  Long term medical...|  2930|2.9007746673583984E7|\n",
      "|                      Debt forgiveness|  1788|1.3169610816955566E7|\n",
      "|                     Food and Beverage|   968|  2285237.6763916016|\n",
      "|                                  Gift|   630|  3701991.0388183594|\n",
      "|                          Acquisitions|   563| 7.191250424938965E7|\n",
      "|                  Charitable Contri...|   239|   7394503.872680664|\n",
      "|                         Entertainment|    30|   61021.20031738281|\n",
      "+--------------------------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_df = g_df.orderBy(\"count\", ascending  = False)\n",
    "g_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccaad75d-2cf6-4265-9038-8136a55bbdc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------+--------------------+\n",
      "|Nature_of_Payment_or_Transfer_of_Value| count|    total_amount_sum|\n",
      "+--------------------------------------+------+--------------------+\n",
      "|                  Compensation for ...|164093| 5.582959709562378E8|\n",
      "|                        Consulting Fee|105239| 4.819268881451416E8|\n",
      "|                    Travel and Lodging| 24793| 5.476596304553223E7|\n",
      "|                             Honoraria| 13750| 5.311015340332031E7|\n",
      "|                             Education| 13376| 5.179660454785156E7|\n",
      "|                    Royalty or License| 11538| 1.190650138420227E9|\n",
      "|                  Compensation for ...|  8658| 2.762957946057129E7|\n",
      "|                                 Grant|  4922|1.1127899889367676E8|\n",
      "|                  Space rental or f...|  4917| 2.768891679724121E7|\n",
      "|                  Long term medical...|  2930|2.9007746673583984E7|\n",
      "+--------------------------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_df.limit(10).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e639a34c-9f7c-4839-94c1-a3ad9f8b3972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------+--------------------+\n",
      "|Nature_of_Payment_or_Transfer_of_Value| count|    total_amount_sum|\n",
      "+--------------------------------------+------+--------------------+\n",
      "|                    Royalty or License| 11538| 1.190650138420227E9|\n",
      "|                  Compensation for ...|164093| 5.582959709562378E8|\n",
      "|                        Consulting Fee|105239| 4.819268881451416E8|\n",
      "|                                 Grant|  4922|1.1127899889367676E8|\n",
      "|                          Acquisitions|   563| 7.191250424938965E7|\n",
      "|                    Travel and Lodging| 24793| 5.476596304553223E7|\n",
      "|                             Honoraria| 13750| 5.311015340332031E7|\n",
      "|                             Education| 13376| 5.179660454785156E7|\n",
      "|                  Long term medical...|  2930|2.9007746673583984E7|\n",
      "|                  Space rental or f...|  4917| 2.768891679724121E7|\n",
      "+--------------------------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_df = g_df.orderBy(\"total_amount_sum\", ascending  = False)\n",
    "g_df.limit(10).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c9d7a6a-3c47-46c6-99bb-9c0b84d1d268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "g_df2 = df.groupBy([F.col(\"Covered_Recipient_Specialty_1\")]).agg(\n",
    "    F.count(F.col(\"Total_Amount_of_Payment_USDollars\")).alias(\"count\"),\n",
    "    F.sum(F.col(\"Total_Amount_of_Payment_USDollars\")).alias(\"total_amount_sum\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f15149-e87e-42e0-bd67-f7906b7483ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+-------+--------------------+\n",
      "|Covered_Recipient_Specialty_1                                                                   |count  |total_amount_sum    |\n",
      "+------------------------------------------------------------------------------------------------+-------+--------------------+\n",
      "|null                                                                                            |30686  |7.936674627555835E8 |\n",
      "|Allopathic & Osteopathic Physicians|Orthopaedic Surgery                                         |210504 |4.0345021277223873E8|\n",
      "|Allopathic & Osteopathic Physicians|Internal Medicine                                           |1307850|1.3136300312194332E8|\n",
      "|Allopathic & Osteopathic Physicians|Psychiatry & Neurology|Neurology                            |457774 |8.979213624269351E7 |\n",
      "|Allopathic & Osteopathic Physicians|Neurological Surgery                                        |78680  |8.608847850082898E7 |\n",
      "|Allopathic & Osteopathic Physicians|Dermatology                                                 |454866 |8.320264770937024E7 |\n",
      "|Allopathic & Osteopathic Physicians|Internal Medicine|Cardiovascular Disease                    |458369 |7.022084117953089E7 |\n",
      "|Allopathic & Osteopathic Physicians|Internal Medicine|Hematology & Oncology                     |307220 |6.946894111538701E7 |\n",
      "|Allopathic & Osteopathic Physicians|Orthopaedic Surgery|Adult Reconstructive Orthopaedic Surgery|17768  |6.6772837355176E7   |\n",
      "|Allopathic & Osteopathic Physicians|Psychiatry & Neurology|Psychiatry                           |307801 |6.329825408609511E7 |\n",
      "+------------------------------------------------------------------------------------------------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_df2 = g_df2.orderBy(\"total_amount_sum\", ascending  = False)\n",
    "g_df2.limit(10).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3085d349-afdc-4808-9c61-c519a508324b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "g_df3 = df.groupBy([F.col(\"Covered_Recipient_Primary_Type_1\")]).agg(\n",
    "    F.count(F.col(\"Total_Amount_of_Payment_USDollars\")).alias(\"count\"),\n",
    "    F.sum(F.col(\"Total_Amount_of_Payment_USDollars\")).alias(\"total_amount_sum\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b522ee4-5f4a-4f5d-bcda-197264ab9705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------+--------------------+\n",
      "|Covered_Recipient_Primary_Type_1      |count  |total_amount_sum    |\n",
      "+--------------------------------------+-------+--------------------+\n",
      "|Medical Doctor                        |7913524|2.0401834645222669E9|\n",
      "|null                                  |30588  |7.933900793164766E8 |\n",
      "|Nurse Practitioner                    |3266415|1.3546701068595254E8|\n",
      "|Doctor of Osteopathy                  |941641 |9.368468839218245E7 |\n",
      "|Doctor of Dentistry                   |350066 |8.315500001068492E7 |\n",
      "|Physician Assistant                   |1618627|6.882057343604396E7 |\n",
      "|Doctor of Podiatric Medicine          |148617 |3.442229348336857E7 |\n",
      "|Doctor of Optometry                   |243641 |2.331802892928546E7 |\n",
      "|Certified Registered Nurse Anesthetist|47191  |2143071.420371622   |\n",
      "|Clinical Nurse Specialist             |27756  |2023960.0999247283  |\n",
      "+--------------------------------------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_df3 = g_df3.orderBy(\"total_amount_sum\", ascending  = False)\n",
    "g_df3.limit(10).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a1511d9-f0f0-4906-a950-e2aeb2555366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3631613044829182,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "createSparkTables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
